{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for cropping, slicing and other data prep operations on ground truth, initial segmentation and image data for error correction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth\n",
    "\n",
    "We slice the ground truth into blocks of the same size as the predicted segmentation. The extents of the ground truth are also cropped to the volume containing all the labeled objects. Parameters for these slicing and cropping operations are globally defined in `data_locs.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate 48 nm downsampled version of GT and find bounding box of labeled volume\n",
    "\n",
    "# gt8nm_file = data_locs[\"gt\"][\"dir\"] + data_locs[\"gt\"][\"8nm\"]\n",
    "# find global bbox of GT segmentation\n",
    "# from skimage import measure\n",
    "# use_global_bbox = True # set to false if you want blockwise bounding boxes\n",
    "#############################\n",
    "# loads entire GT - caution!\n",
    "# this should ideally be a one time evaluation\n",
    "# comment enclosed code and manually set extents below \n",
    "# if global bounding box is known\n",
    "# and if downsampled version is already on disk\n",
    "# gt48nm_full = read3d_h5(gt8nm_file, 'main', dsmpl=(6,6,1))\n",
    "# gt48nm_full = np.swapaxes(gt48nm_full,1,2)\n",
    "# gt48nm_full = np.swapaxes(gt48nm_full,0,1)\n",
    "# fout = data_locs[\"gt\"][\"dir\"] + \"gt48nm_pf-align1_1k.h5\"\n",
    "# writeh5(fout, 'main', gt48nm_full, compression=\"gzip\", \n",
    "#         chunks=(1,gt48nm_full.shape[1],gt48nm_full.shape[2]))\n",
    "# gt_mask = np.zeros_like(gt48nm_full)\n",
    "# gt_mask[gt48nm_full>0] = 1\n",
    "# del gt48nm_full\n",
    "# props = measure.regionprops(gt_mask, cache=True)\n",
    "# del gt_mask\n",
    "# bbox = props[0].bbox\n",
    "# print bbox\n",
    "#############################\n",
    "# bbox = (0, 16, 208, 1001, 556, 696) # save these co-ordinates to data_locs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srujanm/anaconda2/envs/em/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating block 0 from\n",
      "/home/srujanm/cerebellum/data/gt48nm_pf-align1_1k.h5\n",
      "((0, 90), (16, 556), (208, 696))\n",
      "[90, 540, 488]\n",
      "Generating block 1 from\n",
      "/home/srujanm/cerebellum/data/gt48nm_pf-align1_1k.h5\n",
      "((90, 180), (16, 556), (208, 696))\n",
      "[90, 540, 488]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from cerebellum.utils.seg_prep import *\n",
    "\n",
    "with open('data_locs.json') as f:\n",
    "\tdata_locs = json.load(f)\n",
    "\n",
    "resolution = (30, 48, 48)\n",
    "gt48nm_file = data_locs[\"gt\"][\"dir\"] + data_locs[\"gt\"][\"48nm\"]\n",
    "bbox = data_locs[\"gt\"][\"48nm-bbox\"]\n",
    "block_indices = range(2)\n",
    "\n",
    "for i in block_indices:\n",
    "    print \"Generating block %d from\"%(i)\n",
    "    print gt48nm_file\n",
    "    zz = i*data_locs[\"block-size\"]+data_locs[\"aff-offset\"]\n",
    "    gt_block = SegPrep(\"gt%04d\"%(zz), resolution)\n",
    "    block_lims = ((data_locs[\"block-size\"]*i,data_locs[\"block-size\"]*(i+1)),\n",
    "                  (bbox[1],bbox[4]),(bbox[2],bbox[5]))\n",
    "    print block_lims\n",
    "    gt_block.read(gt48nm_file, \"main\", block_lims=block_lims)\n",
    "    print gt_block.shape\n",
    "    gt_block.relabel()\n",
    "    gt_block.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PF segmentation\n",
    "\n",
    "The predicted segmentation needs to be offset and padded with zeros to overlap with the GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating block 0 from\n",
      "/mnt/hp03/donglai/public/cere_pf/data_link_strict/0014.h5\n",
      "((0, 90), (52, 3292), (1204, 4132))\n",
      "[90, 533, 488]\n",
      "Generating block 1 from\n",
      "/mnt/hp03/donglai/public/cere_pf/data_link_strict/0104.h5\n",
      "((0, 90), (52, 3292), (1204, 4132))\n",
      "[90, 533, 488]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from cerebellum.utils.seg_prep import *\n",
    "\n",
    "with open('data_locs.json') as f:\n",
    "\tdata_locs = json.load(f)\n",
    "\n",
    "resolution = (30, 48, 48)\n",
    "dsmpl = (1,6,6)\n",
    "offset = data_locs[\"initial-seg\"][\"8nm-offset\"] # offset aligns segmentation with GT\n",
    "pf8nm_file = data_locs[\"initial-seg\"][\"dir\"] + data_locs[\"initial-seg\"][\"8nm-pf-linked\"]\n",
    "\n",
    "block_indices = range(2)\n",
    "\n",
    "for i in block_indices:\n",
    "    print \"Generating block %d from\"%(i)\n",
    "    zz = i*data_locs[\"block-size\"]+data_locs[\"aff-offset\"]\n",
    "    if zz!=data_locs[\"aff-offset\"]: # adjust block index\n",
    "        pf8nm_file = pf8nm_file[:-7]+\"%04d.h5\"%(zz)\n",
    "    print pf8nm_file\n",
    "    pf_block = SegPrep(\"pred-pf-crop2gt-%04d\"%(zz), resolution)\n",
    "    pf_block_lims = ((0,data_locs[\"block-size\"]),\n",
    "                   (max(0,dsmpl[1]*bbox[1]+offset[1]),dsmpl[1]*bbox[4]+offset[1]),\n",
    "                   (max(0,dsmpl[2]*bbox[2]+offset[2]),dsmpl[2]*bbox[5]+offset[2]))\n",
    "    print pf_block_lims\n",
    "    pf_block.read(pf8nm_file, \"main\", dsmpl=dsmpl, block_lims=pf_block_lims)\n",
    "    print pf_block.shape\n",
    "    pf_block.padzeros((gt_block.shape[0],gt_block.shape[1]-pf_block.shape[1],gt_block.shape[2]), 1)\n",
    "    pf_block.relabel()\n",
    "    pf_block.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 540, 488)\n"
     ]
    }
   ],
   "source": [
    "# generate and save cropped version of image\n",
    "im48nm_file = data_locs[\"image\"][\"dir\"] + data_locs[\"image\"][\"48nm\"]\n",
    "im48nm_block = read3d_h5(im48nm_file, 'main', block_lims=block_lims)\n",
    "print im48nm_block.shape\n",
    "fout = \"/home/srujanm/cerebellum/data/im48nm_cropped_%04d.h5\"%(zz)\n",
    "writeh5(fout, 'main', im48nm_block, compression=\"gzip\", chunks=(1,im48nm_block.shape[1],im48nm_block.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 1620, 1464)\n"
     ]
    }
   ],
   "source": [
    "# repeat for 16 nm res version\n",
    "im16nm_file = data_locs[\"image\"][\"dir\"] + data_locs[\"image\"][\"16nm\"]\n",
    "if zz!=14:\n",
    "     im16nm_file = im16nm_file[:-7]+\"%04d.h5\"%(zz)\n",
    "im_block_lims = ((0,data_locs[\"block-size\"]),\n",
    "                (3*block_lims[1][0],3*block_lims[1][1]),\n",
    "                (3*block_lims[2][0],3*block_lims[2][1]))\n",
    "im16nm_block = read3d_h5(im16nm_file, 'main', block_lims=im_block_lims)\n",
    "print im16nm_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = \"/home/srujanm/cerebellum/data/im16nm_cropped_%04d.h5\"%(zz)\n",
    "writeh5(fout, 'main', im16nm_block, compression=\"gzip\", chunks=(1,im16nm_block.shape[1],im16nm_block.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "im_final = read3d_h5('/home/srujanm/cerebellum/data/im48nm_cropped_%04d.h5'%(zz), 'main')\n",
    "gt_final = read3d_h5('/home/srujanm/cerebellum/data/gt48nm_cropped_pf_%04d.h5'%(zz), 'main')\n",
    "seg_final = read3d_h5('/home/srujanm/cerebellum/data/seg48nm_cropped_pf_%04d.h5'%(zz), 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick two IDs corresponding to the same object from neuroglancer\n",
    "gt_id = 793\n",
    "seg_id = 4012\n",
    "# Plot a section to see if alignment is grossly off\n",
    "zslice = 41\n",
    "mask1 = gt_final[zslice,100:150,160:220]==gt_id\n",
    "mask2 = seg_final[zslice,100:150,160:220]==seg_id\n",
    "mismatch = mask1^mask2\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1,figsize=(15,15))\n",
    "ax1.imshow(mask1)\n",
    "ax2.imshow(mask2)\n",
    "ax3.imshow(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(seg):\n",
    "    \"\"\"\n",
    "    Relabels a segmentation such that max ID = # objects\n",
    "    Args:\n",
    "        seg (ndarray): 3D segmentation\n",
    "    Returns:\n",
    "        seg_relabeled (ndarray)\n",
    "    \"\"\"\n",
    "    seg_ids = get_vols(seg)[0]\n",
    "    n_ids = len(seg_ids)\n",
    "    max_id = np.max(seg_ids)\n",
    "    if max_id == n_ids:\n",
    "        return seg\n",
    "    missing_ids = np.sort(np.array(list(set(range(max_id+1)).difference(set(seg_ids)))))\n",
    "    seg_relabel = seg\n",
    "    for i in range(len(missing_ids)):\n",
    "        if i==len(missing_ids)-1:\n",
    "            ids_to_correct = range(missing_ids[i]+1, max_id+1)\n",
    "        else:\n",
    "            ids_to_correct = range(missing_ids[i]+1, missing_ids[i+1])\n",
    "        for j in ids_to_correct:\n",
    "            seg_relabel[seg==j] = j-(i+1) #TODO (Jeff): speed this up using object-wise bounding boxes\n",
    "    return seg_relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_calc = \n",
    "seg_corrected = relabel(seg_calc)\n",
    "print np.max(seg_corrected), len(get_vols(seg_corrected)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = data_locs[\"trials\"] + \"gt48nm_cropped_relabeled_pf_0014.h5\"\n",
    "writeh5(fout, 'main', seg_calc, compression=\"gzip\", chunks=(1,im16nm_block.shape[1],im16nm_block.shape[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em",
   "language": "python",
   "name": "em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
