{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface slice error detection\n",
    "\n",
    "Goal: See if you can detect merge errors by identifying objects with poor IoU scores in interface slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srujanm/anaconda2/envs/em/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 540, 488)\n",
      "(90, 540, 488)\n"
     ]
    }
   ],
   "source": [
    "from cerebellum.utils.data_io import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('data_locs.json') as f:\n",
    "    data_locs = json.load(f)\n",
    "block_indices = np.array([0, 1]) # TO CHANGE: which blocks are you checking\n",
    "zz = data_locs[\"block-size\"]*block_indices+14\n",
    "\n",
    "# load blocks of interest\n",
    "pred_blocks = len(block_indices)*[None]\n",
    "for i, z in enumerate(zz.tolist()):\n",
    "    pred_file = data_locs[\"trials\"][\"dir\"] + data_locs[\"trials\"][\"pf48nm-cropped-relabeled\"]\n",
    "    if z!=14: # adjust block index\n",
    "        pred_file = pred_file[:-7]+\"%04d.h5\"%(z)\n",
    "    pred_blocks[i] = read3d_h5(pred_file, 'main')\n",
    "    print pred_blocks[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 540, 488) (1, 540, 488)\n",
      "3659 3743\n"
     ]
    }
   ],
   "source": [
    "# extract interface slices\n",
    "last_slice = np.array([pred_blocks[0][data_locs[\"block-size\"]-1,:,:]])\n",
    "first_slice = np.array([pred_blocks[1][0,:,:]])\n",
    "\n",
    "print last_slice.shape, first_slice.shape\n",
    "print np.max(last_slice), np.max(first_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Labels in source slice are not ordered. Relabel and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8985dc36ab8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# IOU CALCULATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minter_iou_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/srujanm/cerebellum/cerebellum/error_analysis/voxel_methods.py\u001b[0m in \u001b[0;36mslice_iou\u001b[0;34m(source_slice, target_slice)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mgood_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgood_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Labels in source slice are not ordered. Relabel and try again'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mn_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Labels in source slice are not ordered. Relabel and try again"
     ]
    }
   ],
   "source": [
    "from cerebellum.error_analysis.voxel_methods import *\n",
    "\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "# IOU CALCULATION\n",
    "ints, unions, ious, orders, runtime = slice_iou(last_slice, first_slice)\n",
    "print runtime\n",
    "inter_iou_results = (ints, unions, ious, orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR DET\n",
    "# find all IDs with IoU below a threshold and object in last_slice bigger than object in first_slice\n",
    "def slice_iou_error_detector(last_slice, first_slice, iou_thresh, iou_results=None, check_order=False):\n",
    "    \"\"\"\n",
    "    Detects all objects in last_slice that have poor IoU with objects in first_slice\n",
    "    If check_order is True, only declares errors in which last_slice object is bigger\n",
    "    \"\"\"\n",
    "    if iou_results is not None:\n",
    "        (ints, unions, ious, orders) = iou_results\n",
    "    else:\n",
    "        ints, unions, ious, orders, _ = slice_iou(last_slice, first_slice)\n",
    "    if check_order:\n",
    "        detected_ids = reduce(np.intersect1d, (np.argwhere(ious<iou_thresh), \n",
    "                                               np.argwhere(ious>0), \n",
    "                                               np.argwhere(orders==True)))\n",
    "    else:\n",
    "        detected_ids = np.intersect1d(np.argwhere(ious<iou_thresh), np.argwhere(ious>0))\n",
    "    return detected_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_thresh = 0.8\n",
    "inter_errors = slice_iou_error_detector(last_slice, first_slice, inter_thresh, iou_results=inter_iou_results, check_order=True)\n",
    "print len(inter_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-block error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOU CALCULATION\n",
    "def slice2slice_iou(seg):\n",
    "    n_slices = seg.shape[0]\n",
    "    n_segs = np.max(seg)\n",
    "    print \"Starting slice-to-slice IoU calculation for %d slices\"%(n_slices)\n",
    "    runtime = 0\n",
    "    iou_results = (n_slices-1)*[None]\n",
    "    for i in range(n_slices-1):\n",
    "        from_slice = np.array([seg[i,:,:]])\n",
    "        to_slice = np.array([seg[i+1,:,:]])\n",
    "        ints, unions, ious, orders, ti = slice_iou(from_slice, to_slice)\n",
    "        iou_results[i] = (ints, unions, ious, orders)\n",
    "        runtime += ti\n",
    "    print \"Runtime: %f\"%(runtime)\n",
    "    return iou_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = pred_blocks[0][:,:,:]\n",
    "intra_iou_results = slice2slice_iou(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR DET\n",
    "def intra_block_error_detector(seg, iou_thresh, iou_results=None):\n",
    "    \"\"\"\n",
    "    Detects all objects for which at least one slice to slice IoU is below iou_thresh\n",
    "    \"\"\"\n",
    "    if iou_results is None:\n",
    "        iou_results = slice2slice_iou(seg)\n",
    "    n_slices = seg.shape[0]\n",
    "    detected_errors = (n_slices-1)*[None]\n",
    "    for i in range(n_slices-1):\n",
    "        from_slice = np.array([seg[i,:,:]])\n",
    "        to_slice = np.array([seg[i+1,:,:]])\n",
    "        slice_results = iou_results[i]\n",
    "        detected_errors[i] = slice_iou_error_detector(from_slice, to_slice, iou_thresh, iou_results=slice_results)\n",
    "    return detected_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_thresh = 0.5\n",
    "intra_block_errors = intra_block_error_detector(seg, intra_thresh, iou_results=intra_iou_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool errors over all slices in one list\n",
    "n_slices = seg.shape[0]\n",
    "pooled_intra_block_errors = []\n",
    "for i in range(n_slices-1):\n",
    "    pooled_intra_block_errors.extend(intra_block_errors[i])\n",
    "pooled_intra_block_errors = list(set(pooled_intra_block_errors))\n",
    "print len(pooled_intra_block_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against GT skeletons\n",
    "\n",
    "The code blocks below are copied from our notebook on [skeleton based error detection](file:///skeleton_error_detection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"pred-all\" # TO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read merge errors identified in GT skeleton analysis\n",
    "def read_merges(read_path):\n",
    "    f = open(read_path + \"/merged.ids\", \"r\")\n",
    "    n_pairs = int(f.readline()) # no of pairs of GT merged skeletons\n",
    "    merge_ids = f.readlines()[::2]\n",
    "    for i, mstr in enumerate(merge_ids):\n",
    "        merge_ids[i] = int(mstr)\n",
    "    merge_ids = list(set(merge_ids))\n",
    "    return merge_ids\n",
    "\n",
    "# read correct IDs identified in GT skeleton analysis\n",
    "def read_corrects(read_path):\n",
    "    f = open(read_path + \"/correct.ids\", \"r\")\n",
    "    n_corr = int(f.readline()) # no of pairs of GT merged skeletons\n",
    "    corr_ids = f.readlines()\n",
    "    for i, cstr in enumerate(corr_ids):\n",
    "        corr_ids[i] = int(cstr.split(',')[1])\n",
    "    return corr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with merge IDs detected from GT skeleton analysis\n",
    "err_analysis_path = 'skeletons/'+prefix+'/error-analysis/'\n",
    "\n",
    "merge_ids = read_merges(err_analysis_path)\n",
    "corr_ids = read_corrects(err_analysis_path)\n",
    "\n",
    "import json\n",
    "with open(err_analysis_path + 'error-analysis-summary.json') as f:\n",
    "    err_summ = json.load(f)\n",
    "assert len(corr_ids) == err_summ[\"results\"][\"correct\"]\n",
    "\n",
    "print \"# merges from GT analysis: %d\"%(len(merge_ids))\n",
    "print \"# corrects from GT analysis: %d\"%(len(corr_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_analysis(detected_ids, merge_ids, corr_ids, write_path=None):\n",
    "    true_pos = list(set(merge_ids)&(set(detected_ids)))\n",
    "    print \"True positives: %d\"%(len(true_pos))\n",
    "    false_pos = list(set(corr_ids)&(set(detected_ids)))\n",
    "    print \"False positives: %d\"%(len(false_pos))\n",
    "    true_neg = list(set(corr_ids).difference(set(detected_ids)))\n",
    "    print \"True negatives: %d\"%(len(true_neg))\n",
    "    false_neg = list(set(merge_ids).difference(set(detected_ids)))\n",
    "    print \"False negatives: %d\"%(len(false_neg))\n",
    "    precision = len(true_pos)/(1.*len(true_pos)+len(false_pos))\n",
    "    recall = len(true_pos)/(1.*len(true_pos)+len(false_neg))\n",
    "    print \"Precision: %f\"%(precision)\n",
    "    print \"Recall: %f\"%(recall)\n",
    "    print \"False pos:\", false_pos\n",
    "    print \"False neg:\", false_neg\n",
    "    if write_path is not None:\n",
    "        np.save(write_path+'false_pos', false_pos)\n",
    "        np.save(write_path+'false_neg', false_neg)\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_ids = list(set(pooled_intra_block_errors)|set(inter_errors))\n",
    "print len(detected_ids)\n",
    "pr_analysis(detected_ids_to_check, merge_ids, corr_ids, write_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take intersection with IDs detected by junction presence in skeleton\n",
    "read_path = 'skeletons/'+prefix+'/error-detection/'\n",
    "sk_true_pos = np.load(read_path+'false_pos.npy')\n",
    "sk_false_pos = np.load(read_path+'true_pos.npy')\n",
    "sk_detected_ids = list(set(sk_true_pos)|set(sk_false_pos))\n",
    "\n",
    "joint_detected_ids = list(set(detected_ids) & set(sk_detected_ids))\n",
    "pr_analysis(joint_detected_ids, merge_ids, corr_ids, write_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR curve sweeping IoU threshold. I ran this experiment quickly by sweeping only the threshold for inter-block part. The threshold for intra-block part was determined manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PR curve sweeping error detection threshold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iou_threshs = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "stitch_det_results = [stitch_error_detector(ints, unions, ious, orders, t) for t in iou_threshs]\n",
    "\n",
    "pr_results = [pr_analysis(result, merge_ids, corr_ids) for result in stitch_det_results]\n",
    "p = [res[0] for res in pr_results]\n",
    "r = [res[1] for res in pr_results]\n",
    "plt.plot(p,r)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em",
   "language": "python",
   "name": "em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
