{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests error detection through checking number of connected components from slie to slice\n",
    "\n",
    "## Error detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srujanm/anaconda2/envs/em/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 540, 488) (90, 540, 488)\n"
     ]
    }
   ],
   "source": [
    "from cerebellum.utils.data_io import *\n",
    "import json\n",
    "\n",
    "with open('data_locs.json') as f:\n",
    "\tdata_locs = json.load(f)\n",
    "block_index = 0 # TO CHANGE: which block you are processing\n",
    "zz = data_locs[\"block-size\"]*block_index+14\n",
    "\n",
    "# load GT and initial segmentation\n",
    "gt_file = data_locs[\"trials\"][\"dir\"] + data_locs[\"trials\"][\"gt48nm-cropped-relabeled\"]\n",
    "pred_file = data_locs[\"trials\"][\"dir\"] + data_locs[\"trials\"][\"pf48nm-cropped-relabeled\"]\n",
    "if zz!=14: # adjust block index\n",
    "    gt_file = gt_file[:-7]+\"%04d.h5\"%(zz)\n",
    "    pred_file = pred_file[:-7]+\"%04d.h5\"%(zz)\n",
    "gt = read3d_h5(gt_file, 'main')\n",
    "pred = read3d_h5(pred_file, 'main')\n",
    "print gt.shape, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660\n"
     ]
    }
   ],
   "source": [
    "n_objs = np.max(pred)+1\n",
    "print n_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 10, 14)\n",
      "2\n",
      "0.166497945786\n"
     ]
    }
   ],
   "source": [
    "# test for single object\n",
    "import time\n",
    "\n",
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "start_time = time.time()\n",
    "obj_id = 1073\n",
    "obj_mask = np.zeros_like(pred)\n",
    "obj_mask[pred==obj_id] = 1\n",
    "\n",
    "bbox = regionprops(obj_mask)[0].bbox\n",
    "obj_mask = obj_mask[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]]\n",
    "print obj_mask.shape\n",
    "\n",
    "slice_id = 62\n",
    "slice_cc = label(obj_mask[slice_id,:,:], connectivity=1)\n",
    "n_cc = np.max(slice_cc)\n",
    "print n_cc\n",
    "print time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "import time\n",
    "\n",
    "def cc_measure(seg, connectivity=2, write_path=None):\n",
    "    \"\"\"\n",
    "    Gets # connected components across z-slices for all objects in segmentation\n",
    "    TODO: make more efficient with bounding boxes\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    n_objs = np.max(seg)+1\n",
    "    n_slices = seg.shape[0]\n",
    "    n_cc = np.zeros((n_objs, n_slices))\n",
    "    print \"checking CC rule for %d objects\"%(n_objs)\n",
    "    # go over each object, skip 0\n",
    "    for obj_id in range(1,n_objs): \n",
    "        print \"on obj %d\"%obj_id\n",
    "        obj_mask = np.zeros_like(seg)\n",
    "        obj_mask[seg==obj_id] = 1\n",
    "        if np.sum(obj_mask)==0: # ignore non-existent objects\n",
    "            continue\n",
    "        regions = regionprops(obj_mask)\n",
    "        bbox = regions[0].bbox\n",
    "        obj_mask = obj_mask[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]]\n",
    "        # go over each slice\n",
    "        check_slices = obj_mask.shape[0] # n_slices\n",
    "        for slice_id in range(check_slices):\n",
    "            n_cc[obj_id, slice_id] = np.max(label(obj_mask[slice_id,:,:], connectivity=connectivity))\n",
    "    print time.time() - start_time\n",
    "    if write_path is not None:\n",
    "        np.save(write_path, n_cc)\n",
    "    return n_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cc_error_detector(n_cc):\n",
    "    \"\"\"\n",
    "    Checks if CC # is conserved across all slices, returns obj_ids that are in violation\n",
    "    \n",
    "    n_cc (n_objs x n_slices ndarray): \n",
    "    \"\"\"\n",
    "    n_objs = n_cc.shape[0]\n",
    "    ## check if CC # is conserved across all slices\n",
    "    # flag_cc = [obj_id for obj_id in range(n_objs) if not np.all(n_cc[obj_id,:]==n_cc[obj_id,0])]\n",
    "    \n",
    "    # check if CC # is 1 across all slices\n",
    "    flag_cc = [obj_id for obj_id in range(n_objs) if not np.all(n_cc[obj_id,:]==1)]\n",
    "    return flag_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking CC rule for 3660 objects\n",
      "on obj 1\n",
      "on obj 2\n",
      "on obj 3\n",
      "on obj 4\n",
      "on obj 5\n",
      "on obj 6\n",
      "on obj 7\n",
      "on obj 8\n",
      "on obj 9\n",
      "on obj 10\n",
      "on obj 11\n",
      "on obj 12\n",
      "on obj 13\n",
      "on obj 14\n",
      "on obj 15\n",
      "on obj 16\n",
      "on obj 17\n",
      "on obj 18\n",
      "on obj 19\n",
      "on obj 20\n",
      "on obj 21\n",
      "on obj 22\n",
      "on obj 23\n",
      "on obj 24\n",
      "on obj 25\n",
      "on obj 26\n",
      "on obj 27\n",
      "on obj 28\n",
      "on obj 29\n",
      "on obj 30\n",
      "on obj 31\n",
      "on obj 32\n",
      "on obj 33\n",
      "on obj 34\n",
      "on obj 35\n",
      "on obj 36\n",
      "on obj 37\n",
      "on obj 38\n",
      "on obj 39\n",
      "on obj 40\n",
      "on obj 41\n",
      "on obj 42\n",
      "on obj 43\n",
      "on obj 44\n",
      "on obj 45\n",
      "on obj 46\n",
      "on obj 47\n",
      "on obj 48\n",
      "on obj 49\n",
      "on obj 50\n",
      "on obj 51\n",
      "on obj 52\n",
      "on obj 53\n",
      "on obj 54\n",
      "on obj 55\n",
      "on obj 56\n",
      "on obj 57\n",
      "on obj 58\n",
      "on obj 59\n",
      "on obj 60\n",
      "on obj 61\n",
      "on obj 62\n",
      "on obj 63\n",
      "on obj 64\n",
      "on obj 65\n",
      "on obj 66\n",
      "on obj 67\n",
      "on obj 68\n",
      "on obj 69\n",
      "on obj 70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c351423152f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./err-correction/pred-all/ncc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-adfb15e14bcc>\u001b[0m in \u001b[0;36mcc_measure\u001b[0;34m(seg, connectivity, write_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mobj_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_objs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"on obj %d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobj_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mobj_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# ignore non-existent objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/srujanm/anaconda2/envs/em/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_cc = cc_measure(pred, connectivity=2, write_path='./err-correction/pred-all/ncc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_ids = cc_error_detector(n_cc)\n",
    "passed_ids = list(set(range(n_objs)).difference(set(detected_ids)))\n",
    "print len(detected_ids)\n",
    "print len(passed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected component properties of detected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many objects are merged in each detected object\n",
    "n_merge = [n_cc[i,:].max() for i in detected_ids]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(x=n_merge, bins=np.arange(9)-0.5, rwidth=0.5)\n",
    "plt.xlabel('Max CC #')\n",
    "plt.ylabel('Number of objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the detected objects have max CC # of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cc_binning(n_cc):\n",
    "    \"\"\"\n",
    "    Sorts objects into bins based on max CC # over all slices\n",
    "    \"\"\"\n",
    "    n_objs = n_cc.shape[0]\n",
    "    max_cc = np.max(n_cc) # max CC's per slice across all objects\n",
    "    cc_binned_objs = [[] for _ in range(max_cc+1)]\n",
    "    for i in range(n_objs):\n",
    "        cc_binned_objs[np.max(n_cc[i,:])].append(i)\n",
    "    return cc_binned_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cc = n_cc.astype(np.int)\n",
    "objs_binned = cc_binning(n_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects whose max CC # is 1, but do not extend through all slices\n",
    "chopped_objs = list(set(objs_binned[1]).difference(passed_ids))\n",
    "print len(chopped_objs)\n",
    "print chopped_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above objects fall in the `1` bin of the histogram above. They are mostly at the boundaries of the imaging volume. As a result, they do not extend through all slices. They can safely be ignored for error correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-object merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what fraction of slices contain 2 CC's?\n",
    "split_frac = [n_cc[i,:].tolist().count(2) for i in objs_binned[2]]\n",
    "\n",
    "plt.hist(x=split_frac, bins=pred.shape[0]/10)\n",
    "plt.xlabel('# slices with 2 CCs')\n",
    "plt.ylabel('Number of objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the objects have <10 slices out of the total 90 in the block with 2 connected components. The rest of the slices are either merges or single object slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print objs_binned[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate against GT skeletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebellum.error_analysis.skel_segeval import SkelEval\n",
    "\n",
    "i = 0 # BLOCK INDEX - to change or iterate on\n",
    "zz = i*data_locs[\"block-size\"]+data_locs[\"aff-offset\"]\n",
    "gt_name = \"gt%04d\"%(zz)\n",
    "pred_name = \"pred-pf-crop2gt-%04d\"%(zz)\n",
    "skel_eval = SkelEval(gt_name, pred_name)\n",
    "merge_ids = skel_eval.get_merges(look_in=\"pred\")\n",
    "corr_ids = skel_eval.get_corrects()\n",
    "print \"No. of falsely merged pred objects (from error analysis): %d\"%(len(merge_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_analysis(detected_ids, merge_ids, corr_ids, write_path=None):\n",
    "    true_pos = list(set(merge_ids)&(set(detected_ids)))\n",
    "    print \"True positives: %d\"%(len(true_pos))\n",
    "    false_pos = list(set(corr_ids)&(set(detected_ids)))\n",
    "    print \"False positives: %d\"%(len(false_pos))\n",
    "    true_neg = list(set(corr_ids).difference(set(detected_ids)))\n",
    "    print \"True negatives: %d\"%(len(true_neg))\n",
    "    false_neg = list(set(merge_ids).difference(set(detected_ids)))\n",
    "    print \"False negatives: %d\"%(len(false_neg))\n",
    "    precision = len(true_pos)/(1.*len(true_pos)+len(false_pos))\n",
    "    recall = len(true_pos)/(1.*len(true_pos)+len(false_neg))\n",
    "    print \"Precision: %f\"%(precision)\n",
    "    print \"Recall: %f\"%(recall)\n",
    "    print \"False pos:\", false_pos\n",
    "    print \"False neg:\", false_neg\n",
    "    if write_path is not None:\n",
    "        np.save(write_path+'false_pos', false_pos)\n",
    "        np.save(write_path+'false_neg', false_neg)\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_analysis(detected_ids, merge_ids, corr_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above precision estimate is a lower bound, because the false positive list above has many objects that are merges between labeled and unlabeled objects in GT. To get a more reasonable precision estimate, wethrow out those objects that have a thresholded fraction of their voxels in GT 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebellum.error_analysis.voxel_methods import intersection_list\n",
    "\n",
    "def filter_detector(gt, pred, detected_ids, thresh=0.4):\n",
    "    filtered_ids = detected_ids[:]\n",
    "    for obj_id in detected_ids:\n",
    "        gt_ids, gt_vols = intersection_list(pred, gt, obj_id) # note reversed order compared to function args\n",
    "        if 0 in gt_ids.tolist():\n",
    "            zero_content = float(gt_vols[gt_ids==0][0])/np.sum(gt_vols)\n",
    "            #print obj_id, zero_content\n",
    "            if zero_content > thresh:\n",
    "                filtered_ids.remove(obj_id)\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better lower bound for precision, reduce `thresh` until precision increases but recall remains close to that of unfiltered set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_ids_val = filter_detector(gt, pred, detected_ids, thresh=0.4)\n",
    "print len(detected_ids_val)\n",
    "pr_analysis(detected_ids_val, merge_ids, corr_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, many of these 'false positives' still seem to be labeled-unlabeled merges, so our precision is likely much higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error correction of a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cc= np.load('./err-correction/pred-all/ncc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_id = 2252\n",
    "n_slices = pred.shape[0]\n",
    "plt.scatter(np.arange(n_slices), n_cc[test_id,:], c=\"g\", alpha=0.5)\n",
    "plt.xlabel(\"slice id\")\n",
    "plt.ylabel(\"CC #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"pred-all\" # TO CHANGE\n",
    "output_resolution=(80,80,80)\n",
    "from cerebellum.ibex.utilities.dataIO import ReadSkeletons\n",
    "skeletons = ReadSkeletons(prefix, downsample_resolution=output_resolution, read_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot skeleton\n",
    "plot_sk = skeletons[test_id]\n",
    "plot_sk.save_image('./err-correction/'+prefix+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skel_flow(point, skeleton):\n",
    "    \"\"\"\n",
    "    returns best non-horizontal flow vector at point based on nearest skeleton edge\n",
    "    \n",
    "    vector is normalized such that z-component = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def dist_pt2ln(pt, ln):\n",
    "        \"\"\"\n",
    "        finds euclidean distance between point and line\n",
    "        Args:\n",
    "            pt (3x, array)\n",
    "            ln (2x3 array)\n",
    "        \"\"\"\n",
    "        p1 = ln[0,:]\n",
    "        p2 = ln[1,:]\n",
    "        return np.linalg.norm(np.cross(p2-p1, p1-pt))/np.linalg.norm(p2-p1)\n",
    "    \n",
    "    try:\n",
    "        assert type(point) is list and len(point)==3\n",
    "    except:\n",
    "        print point\n",
    "    nodes = skeleton.get_nodes()\n",
    "    dists = np.array([np.linalg.norm(np.array(node-point)) for node in nodes])\n",
    "    nodes = [nodes[i] for i in np.argsort(dists)]\n",
    "    #print nodes\n",
    "    edges = skeleton.get_edges()\n",
    "    edge_vecs = [edge[0]-edge[1] for edge in edges]\n",
    "    edge_vecs = [np.divide(edge_vec, float(edge_vec[0])) for edge_vec in edge_vecs] # normalize such that z-comp is 1\n",
    "    # iterate over nodes till you find a non-horizontal edge at a node closest to point\n",
    "    found_flow = False\n",
    "    check_edges = []\n",
    "    while not found_flow:\n",
    "        try:\n",
    "            check_node = nodes.pop(0) # closest node to input point\n",
    "            #print check_node\n",
    "        except:\n",
    "            print \"Error! Could not find non-horizontal flow vector at this point\"\n",
    "        for i, (edge, edge_vec) in enumerate(zip(edges, edge_vecs)):\n",
    "            allow_edge = ((np.all(check_node==edge[0]) or np.all(check_node==edge[1])) \n",
    "                             and not np.any(np.isnan(edge_vec)) and not np.any(np.isinf(edge_vec)))\n",
    "            if allow_edge:\n",
    "                check_edges.append(i)\n",
    "        found_flow = (len(check_edges)>0)\n",
    "\n",
    "    #print edges[check_edges]\n",
    "    #print check_node\n",
    "    # find edges for which the point is z-midway\n",
    "    midway_edges = []\n",
    "    for i in check_edges:\n",
    "        z_lower = min(edges[i][0,0], edges[i][1,0])\n",
    "        z_higher = max(edges[i][0,0], edges[i][1,0])\n",
    "        if z_lower > point[0] or z_higher < point[0]:\n",
    "            midway_edges.append(i)\n",
    "    if len(midway_edges)>0: \n",
    "        check_edges = midway_edges\n",
    "    # find edge closest to point\n",
    "    check_dists = [dist_pt2ln(np.array(point), edges[i]) for i in check_edges]\n",
    "    flow_edge_id = check_edges[np.argmin(check_dists)]\n",
    "    #print edges[flow_edge_id]\n",
    "    flow_vec = edge_vecs[flow_edge_id]\n",
    "    return flow_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point = [38,22+bbox[1],7+bbox[2]]\n",
    "# print point\n",
    "# skel_flow(point, skeletons[test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load high resolution segmentation for splitting\n",
    "pred_file = './segs/'+'pred-pf-8nm-crop2gt-%04d/'%(zz)+'seg.h5'\n",
    "pred = read3d_h5(pred_file, 'main')\n",
    "print pred.shape\n",
    "hres = (30,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import regionprops\n",
    "\n",
    "test_obj_mask = np.zeros_like(pred)\n",
    "test_obj_mask[pred==test_id] = 1\n",
    "regions = regionprops(test_obj_mask)\n",
    "bbox = regions[0].bbox\n",
    "print bbox\n",
    "test_obj_mask = test_obj_mask[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]]\n",
    "print test_obj_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import label\n",
    "\n",
    "from cerebellum.utils.data_io import create_folder\n",
    "# generate labels for CC's in every slice\n",
    "check_slices = test_obj_mask.shape[0] # n_slices\n",
    "labeled_obj_mask = np.zeros_like(test_obj_mask)\n",
    "fpath = './err-correction/'+prefix+'/labeled-obj-mask-8nm-%d/'%(test_id)\n",
    "create_folder(fpath)\n",
    "\n",
    "# save plots of slices\n",
    "plt.ioff()\n",
    "for slice_id in range(check_slices):\n",
    "    labeled_obj_mask[slice_id,:,:] = label(test_obj_mask[slice_id,:,:], connectivity=2)\n",
    "    fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "    ax.imshow(labeled_obj_mask[slice_id,:,:])\n",
    "    plt.savefig(fpath+'slice%d.png'%(slice_id))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test watershed based splitting of a slice with a false merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "def split_slice(fill_slice, seed_method=\"skel-flow\",\n",
    "                footprint=None, \n",
    "                pivot_slice=None, pivot_id=None, fill_dir=None, bbox_offset=None, \n",
    "                seg_res = None, skeleton=None,\n",
    "                prop_from=\"dist-xform\", tweak_boundary_seeds=True, min_seed_connectivity=1,\n",
    "                save_path=None, plot_seeds=False):\n",
    "    \n",
    "    import itertools\n",
    "    \n",
    "    def snap(pt, image):\n",
    "        \"\"\"\n",
    "        Checks if pt lies outside image extents, snaps to nearest pt inside image grid\n",
    "        Args:\n",
    "            pt (Nx, array)\n",
    "            image (N dim array)\n",
    "        \"\"\"\n",
    "        new_pt = list(pt)\n",
    "        for i in range(len(image.shape)):\n",
    "            if pt[i]<0:\n",
    "                new_pt[i] = 0\n",
    "            elif pt[i]>=image.shape[i]:\n",
    "                new_pt[i] = image.shape[i]-1\n",
    "        return tuple(new_pt)\n",
    "    \n",
    "    def on_boundary(pt, image, connectivity=2):\n",
    "        \"\"\"\n",
    "        Checks if pt is on a boundary in a binary image\n",
    "        Args:\n",
    "            pt (2x, array)\n",
    "            image (2 dim array)\n",
    "            connectivity (int)\n",
    "        \"\"\"\n",
    "        assert len(pt)==2\n",
    "        assert len(np.unique(image))==2\n",
    "        assert connectivity==1 or connectivity==2\n",
    "        # generate neighbors\n",
    "        if connectivity==2:\n",
    "            neighbs = [[(pt[0]+d0, pt[1]+d1) for d0 in [-1,0,1]] for d1 in [-1,0,1]]\n",
    "            neighbs = list(itertools.chain.from_iterable(neighbs))\n",
    "            neighbs.remove(pt)\n",
    "        elif connectivity==1:\n",
    "            neighbs = [(pt[0]+1,pt[1]), (pt[0]-1,pt[1]), (pt[0],pt[1]-1), (pt[0],pt[1]+1)]\n",
    "        neighbs = [snap(nb, image) for nb in neighbs]\n",
    "        if any([image[nb]==0 for nb in neighbs]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def push_inwards(pt, image, target_conn=2):\n",
    "        \"\"\"\n",
    "        Pushes a pt on boundary of image inwards, tries to return best connected neighbor\n",
    "        Args:\n",
    "            pt (2x, array)\n",
    "            image (2 dim array)\n",
    "        \"\"\"\n",
    "        assert target_conn>0\n",
    "        try:\n",
    "            assert on_boundary(pt, image, connectivity=target_conn)\n",
    "        except:\n",
    "            print \"Point already inwards, returning as is\"\n",
    "            return pt\n",
    "        neighbs = [[(pt[0]+d0, pt[1]+d1) for d0 in [-1,0,1]] for d1 in [-1,0,1]]\n",
    "        neighbs = list(itertools.chain.from_iterable(neighbs))\n",
    "        neighbs.remove(pt)\n",
    "        neighbs = [snap(nb, image) for nb in neighbs]\n",
    "        one_neighbs = [nb for nb in neighbs if not on_boundary(nb, image, connectivity=1)]\n",
    "        two_neighbs = [nb for nb in neighbs if not on_boundary(nb, image, connectivity=2)]\n",
    "        if len(two_neighbs)>0 and target_conn<=2:\n",
    "            return two_neighbs[0] # arbitraily chosen\n",
    "        elif len(two_neighbs)==0 and len(one_neighbs)>0 and target_conn==1:\n",
    "            return one_neighbs[0] # arbitraily chosen\n",
    "        else:\n",
    "            print \"Warning: Failed to find better connected neighbor. Returning point as is\"\n",
    "            return pt\n",
    "    \n",
    "    assert seed_method==\"dist-xform\" or seed_method==\"skel-flow\"\n",
    "    \n",
    "    distance = ndi.distance_transform_edt(fill_slice) # could also use mean affinity map here\n",
    "    \n",
    "    # generate seeds for watershed\n",
    "    # Method 1: local maxima of distance xform of fill slice\n",
    "    if seed_method==\"dist-xform\":\n",
    "        assert type(footprint) is np.ndarray\n",
    "        local_maxi = peak_local_max(distance, indices=False, num_peaks=2, footprint=footprint) # TODO: CCs>2\n",
    "        seeds = ndi.label(local_maxi)[0]\n",
    "        \n",
    "    # Method 2: use flow vector from skeleton\n",
    "    elif seed_method==\"skel-flow\":\n",
    "        assert prop_from==\"centroids\" or prop_from==\"dist-xform\"\n",
    "        assert fill_dir==-1 or fill_dir==1\n",
    "        assert pivot_slice.shape == fill_slice.shape\n",
    "        assert type(pivot_id) is int\n",
    "        assert bbox_offset is not None and len(bbox_offset)==2\n",
    "        assert skeleton is not None\n",
    "        if tweak_boundary_seeds: assert min_seed_connectivity>0\n",
    "        \n",
    "        # Choose points in pivot slice to propagate\n",
    "        max_ccs = np.max(pivot_slice) # TO CHANGE: extend to more CCs\n",
    "        pivot_regions = [np.zeros_like(pivot_slice) for _ in range(max_ccs)]\n",
    "        for i, pr in enumerate(pivot_regions):\n",
    "            pr[pivot_slice==i+1] = 1\n",
    "        # Method 1: centroids\n",
    "        if prop_from==\"centroids\":\n",
    "            pivot_centroids = [np.mean(np.argwhere(pr!=0), axis=0) for pr in pivot_regions]\n",
    "            prop_pts = pivot_centroids\n",
    "        # Method 2: local maxima of distance transform of each CC\n",
    "        elif prop_from==\"dist-xform\":\n",
    "            pivot_distances = [ndi.distance_transform_edt(pr) for pr in pivot_regions]\n",
    "            pivot_dx_centers = [peak_local_max(pd, indices=True, num_peaks=1)[0] for pd in pivot_distances]\n",
    "            prop_pts = pivot_dx_centers\n",
    "        print \"Pivot points for propagation:\", prop_pts\n",
    "        \n",
    "        # Estimate flow\n",
    "        assert skeleton.resolution[0]==seg_res[0]\n",
    "        dsmpl = (seg_res[1]/skeleton.resolution[1], seg_res[2]/skeleton.resolution[2])\n",
    "        flow_sources = [[pivot_id, (p[0]+bbox_offset[0])/dsmpl[0], (p[1]+bbox_offset[1])/dsmpl[1]] \n",
    "                        for p in prop_pts] # repackage for skel_flow()\n",
    "        #print \"Flow sources:\", flow_sources\n",
    "        flow_vectors = [skel_flow(point, skeleton) for point in flow_sources]\n",
    "        print \"Flow vectors:\", [f.tolist() for f in flow_vectors]\n",
    "        \n",
    "        # Generate seeds\n",
    "        seed_locs = [(int(p[0]+fill_dir*f[1]), \n",
    "                    int(p[1]+fill_dir*f[2])) for (p, f) in zip(prop_pts, flow_vectors)]\n",
    "        print \"Seed locs:\", seed_locs\n",
    "        seeds = np.zeros_like(fill_slice)\n",
    "        seed_validity = [True for _ in range(max_ccs)]\n",
    "        for i, seed_loc in enumerate(seed_locs):\n",
    "            if fill_slice[seed_loc]==0:\n",
    "                print \"Warning: Skel-flow seed generated in empty region of fill slice. Ignoring and proceeding with watershed\"\n",
    "            # TODO: Re-enable tweak_boundary options after fixing on_boundary and push_inwards to operate on images with >1 CC\n",
    "            #elif on_boundary(seed_loc, fill_slice, connectivity=min_seed_connectivity):\n",
    "            #    if tweak_boundary_seeds:\n",
    "            #        print \"Warning: Skel-flow seed on boundary of fill slice. Will attempt to push inwards\"\n",
    "            #        seed_loc = push_inwards(seed_loc, fill_slice, target_conn=min_seed_connectivity)\n",
    "            #    else:\n",
    "            #        print \"Warning: Skel-flow seed on boundary of fill slice. Ignoring and proceeding with watershed\"\n",
    "            seeds[seed_loc] = i+1\n",
    "\n",
    "    # run watershed\n",
    "    filled_slice = watershed(-distance, seeds, mask=fill_slice)\n",
    "    # plot original slice with seeds and watershedded slice for debugging\n",
    "    if plot_seeds:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,15))\n",
    "        ax1.imshow(fill_slice, alpha=0.5)\n",
    "        ax1.imshow(seeds, alpha=0.5)\n",
    "        ax2.imshow(distance)\n",
    "        ax3.imshow(filled_slice)\n",
    "        plt.show()\n",
    "    # plot watershedded slice only\n",
    "    if save_path is not None:\n",
    "        fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "        ax.imshow(filled_slice)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    return filled_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot_id = 82 # correct slice\n",
    "f = 1 # flow dir\n",
    "fill_id = pivot_id + f # ID of slice to work on\n",
    "pivot_slice = labeled_obj_mask[pivot_id,:,:].copy()\n",
    "fill_slice = labeled_obj_mask[fill_id,:,:].copy()\n",
    "filled_slice = split_slice(fill_slice, \n",
    "                           seed_method=\"skel-flow\", \n",
    "                           pivot_slice=pivot_slice, pivot_id=pivot_id,\n",
    "                           fill_dir=f, bbox_offset=(bbox[1], bbox[2]), \n",
    "                           seg_res=hres, skeleton=skeletons[test_id],\n",
    "                           prop_from=\"dist-xform\", tweak_boundary_seeds=False,\n",
    "                           plot_seeds=True)\n",
    "#filled_slice = split_slice(fill_slice, seed_method=\"dist-xform\", footprint=np.ones((5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test iterative watershed based splitting for a neighborhood of incorrect slices starting from a pivotal slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pivotal slices and fill directions\n",
    "pivot_slices = []\n",
    "fill_dirs = []\n",
    "for i in range(n_slices):\n",
    "    if n_cc[test_id,i]==2:\n",
    "        if i>0 and n_cc[test_id, i-1]==1:\n",
    "            pivot_slices.append(i)\n",
    "            fill_dirs.append(-1)\n",
    "        if i<n_slices-1 and n_cc[test_id, i+1]==1:\n",
    "            pivot_slices.append(i)\n",
    "            fill_dirs.append(1)\n",
    "        if len(pivot_slices)>2 and pivot_slices[-1]==pivot_slices[-2]==i:\n",
    "            del pivot_slices[-1]\n",
    "            del fill_dirs[-1]\n",
    "            fill_dirs[-1]=0\n",
    "print zip(pivot_slices, fill_dirs)\n",
    "# generate fill neighborhoods\n",
    "# each neighborhood is of the form [pivotal slice, terminal slice]\n",
    "fill_nbds = []\n",
    "for i in range(len(pivot_slices)):\n",
    "    if fill_dirs[i]==-1 or fill_dirs[i]==0:\n",
    "        if i==0:\n",
    "            fill_nbds.append([pivot_slices[i], 0])\n",
    "        else:\n",
    "            fill_nbds.append([pivot_slices[i], (pivot_slices[i-1]+pivot_slices[i])/2+1])\n",
    "    if fill_dirs[i]==1 or fill_dirs[i]==0:\n",
    "        if i==len(pivot_slices)-1:\n",
    "            fill_nbds.append([pivot_slices[i], len(pivot_slices)])\n",
    "        else:\n",
    "            fill_nbds.append([pivot_slices[i], (pivot_slices[i+1]+pivot_slices[i])/2])\n",
    "print fill_nbds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_object(labeled_obj_mask, fill_nbds, bbox_offset, seg_res, skeleton, save_path):\n",
    "    fill_nbds_updated = fill_nbds[:]\n",
    "    for fill_nbd in fill_nbds_updated:\n",
    "        print \"\\n\\nSplitting slices in the neighborhood:\", fill_nbd\n",
    "        corrected_mask = labeled_obj_mask.copy()\n",
    "        while fill_nbd[0]!=fill_nbd[1]: # till all slices in neighborhood are filled\n",
    "            pivot_id = fill_nbd[0]\n",
    "            pivot_slice = corrected_mask[pivot_id,:,:]\n",
    "            fill_dir = int(np.sign(fill_nbd[1]-fill_nbd[0]))\n",
    "            fill_id = fill_nbd[0]+fill_dir\n",
    "            print \"\\nTracking %d and splitting %d\"%(pivot_id, fill_id)\n",
    "            fill_slice = corrected_mask[fill_id,:,:]\n",
    "            plot_seeds = False\n",
    "            if pivot_id in range(16,21):\n",
    "                plot_seeds = True\n",
    "            corrected_mask[fill_id,:,:] =  split_slice(fill_slice, seed_method=\"skel-flow\", \n",
    "                                                       pivot_slice=pivot_slice, pivot_id=pivot_id,\n",
    "                                                       fill_dir=fill_dir, bbox_offset=bbox_offset, \n",
    "                                                       seg_res=seg_res, skeleton=skeleton,\n",
    "                                                       prop_from=\"dist-xform\", \n",
    "                                                       tweak_boundary_seeds=False, min_seed_connectivity=1,\n",
    "                                                       save_path=save_path+'slice%d.png'%(fill_id),\n",
    "                                                       plot_seeds=plot_seeds)\n",
    "            fill_nbd[0] += fill_dir # advance pivot slice in filling direction\n",
    "    return corrected_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spath = './err-correction/pred-all/split-obj-mask-8nm-%d/'%(test_id)\n",
    "create_folder(spath)\n",
    "# import sys\n",
    "\n",
    "# orig_stdout = sys.stdout\n",
    "# f = open(fpath + 'split.log', 'w')\n",
    "# sys.stdout = f\n",
    "\n",
    "corrected_mask = split_object(labeled_obj_mask, fill_nbds, (bbox[1], bbox[2]),\n",
    "                              hres, skeletons[test_id], \n",
    "                              spath)\n",
    "\n",
    "# sys.stdout = orig_stdout\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em",
   "language": "python",
   "name": "em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
